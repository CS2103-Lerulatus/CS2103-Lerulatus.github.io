# -*- coding: utf-8 -*-
"""CS3244 Tutorial 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iTiMuKdKRlDmuqSrjt7P3BBQD8NKT2x2

# CS3244 Tutorial 2

## **Problem 1** - Yet Another Method To Minimize Functions

Optimization is an essential component of machine learning. As we have seen in linear regression, we need to find the weight vector $w$ that minimizes the error $E(w)$. This will be a recurring theme in many machine learning algorithm: we construct some error function and we want to find a some parameter $w$ that minimizes the error. In the case of mean squared error, we are lucky to find a closed-form solution $\hat{w}=(X^\top X)^{-1}X^\top t$ where $X\in\mathbb{R}^{N\times D}$, but our luck quickly runs out when we start dealing with more complicated error functions, as you will quickly see next week. As such, let us take some time to develop yet another method for finding a weight vector $w$ that minimizes an arbitary function $E(w)$.

### Problem 1.1

Let us develop some intuition. Suppose you are on a foggy and silent hill where your visibility is only limited to a small radius, and your life depends on climbing down the hill to safety. Which direction would you instinctively take to save yourself from the [Pyramid Head](https://silenthill.fandom.com/wiki/Pyramid_Head)?

### Problem 1.2

Let $E(w)$ be an arbitary differentiable function in $w$. Formalize the developed intuition into an iterative algorithm. Discuss the potential drawbacks of this algorithm and suggest potential solutions for them.

### Problem 1.3

Even if we restrict ourselves to linear regression, there is still a case to be made against solving $\hat{w}=(X^\top X)^{-1}X^\top t$ analytically. Let $A\in\mathbb{R}^{n\times n}$. We know that the time complexity for solving $A^{-1}$ naively is $\mathcal{O}(n^3)$.

1. What is the time complexity for solving $\hat{w}$ analytically?
2. In particular, how does it scales with $N$ and $D$?
3. How does this compares with gradient descent?

## **Problem 2** - Norms and Regularizers

We love to formalize everything in mathematics. We have seen such an act of formalization in Lecture 2 when we talked about probability spaces; here, we are going to formalize the notion of length and distance.

Let us start with length. Suppose $V$ is a set of elements on which we want to have imbue a notion of length (concretely $V$ can be the vector space $\mathbb{R}^n$). In other words, there exists some length function $\|\cdot\|$ such that for each element $v\in V$, $\|v\|$ is the length of $v$. Mathematically, we say that $\|\cdot\|$ is **a norm defined on** the set $V$, $\|v\|$ is **the norm of** the element $v\in V$, and the tuple $(V,\|\cdot\|)$ is a normed vector space. Furthermore, a norm $\|\cdot\|$ has to satisfy the following properties.

1. $\|v\|\geq0$ for all $v\in V$ (non-negativity).
2. $\|v\|=0\implies v=0$ for all $v\in V$ (positive definiteness).
3. $\|av\|=|a|\cdot\|v\|$ for all scalar $a\in\mathbb{R}$ and $v\in V$ (homogeneity).
4. $\|v+u\|\leq\|v\|+\|u\|$ for all $v,u\in V$ (triangle inequality).

### Problem 2.1

Let $V=\mathbb{R}^n$ be a vector space. For each $v\in V$, the $L_2$-norm $\|\cdot\|_2$ is defined as

$$\|v\|_2=\left(\sum_{i=1}^n|v_i|^2\right)^{1/2}.$$

Show that $\|\cdot\|_2$ is a norm.

### Problem 2.2

Let $V=\mathbb{R}^n$ be a vector space. For each $v\in V$, the $L_1$-norm $\|\cdot\|_1$ is defined as

$$\|v\|_1=\sum_{i=1}^n|v_i|.$$

Show that $\|\cdot\|_1$ is a norm.

> **Fun (but [definitely useless](https://media.makeameme.org/created/sus-60gw8n.jpg) for this module) fact.** We can also write the $L_1$-norm as
>
> $$\|v\|_1=\sum_{i=1}^nv_i=\left(\sum_{i=1}^n|v_i|^1\right)^{1/1}.$$
>
> This is oddly similar to the $L_2$-norm, but with 2 replaced with 1. This should tickle your generalizing cells: what if we use some number $p$ instead of 1 or 2. It turns out that for $p\geq1$, the natural extension of this equation is in fact a norm. In other words, we can define the $L_p$-norm as:
>
> $$\|v\|_p=\left(\sum_{i=1}^n|v_i|^p\right)^{1/p}.$$
>
> We have a truly remarkable proof to show that this is a norm which our margin (of time) is too small to contain.

### Problem 2.3

Using [Desmos](https://www.desmos.com/calculator) or otherwise, graph out $|x_1|^k+|x_2|^k=1$ where $k$ should be an interactive slider. Hypothesize about the shape and the mathematical expression when $k\to\infty$.

## **Problem 3** - Bias Variance Decomposition

Till now, all the learning algorithms we saw seek to find a linear predictor $y$ that minimizes the some error function over our dataset $D$: $y=\arg\min_{y'}E(y';D)$. For instance, in linear regression, this error function is

$$E(y';D)=\frac{1}{|D|}\sum_{n=1}^{|D|}(y'(x_n)-t_n)^2.$$

Recall that minimizing this training error is not our ultimate goal; what we really want to achieve is good generalization error. In other words, we want to perform well on test examples $(x,t)\sim p(x,t)$ that are not seen in the training set. Mathematically, this means that we are interested in obtaining low generalization loss $\mathbb{E}_{x,t,D}[(y(x;D)-t)^2]$. We have seen in lecture that this can be decomposed into three terms:

$$\mathbb{E}_{x,t,D}\left[(y(x;D)-t)^2\right]=\underbrace{\mathbb{E}_{x,D}\left[(y(x;D)-\bar{y}(x))^2\right]}_{\text{variance}}+\underbrace{\mathbb{E}_{x}\left[(\bar{y}(x)-\bar{t}(x))^2\right]}_{\text{bias}}+\underbrace{\mathbb{E}_{x,t}\left[(\bar{t}(x)-t)^2\right]}_{\text{noise}}.$$

In this problem, we shall explore this decomposition in more depth.

### Problem 3.1

In plain ol' linear regression, we previously derived that the predictor

$$y(x;D)=x^\top\underbrace{(X_D^\top X_D)^{-1}X_D^\top t_D}_{w_D}$$

minimizes the mean squared error $E(y,D)=E_{\text{mse}}(y,D)$. We denote entities that depend on the training set $D$ with a subscript $D$ for clarity. Assume $t(x)=w_*^\top x+\epsilon$ where $\epsilon\sim\mathcal{N}(0,1)$.

1. Explain why $\bar{t}(x)=x^\top w_*$.
2. Explain why $t_D=X_D w_*+\vec{\epsilon}$ where $\vec{\epsilon}\sim\mathcal{N}(0,I)$.
3. Using part (2) or otherwise, show that $\bar{y}(x)=x^\top w_*$.
4. Explain why the bias is zero when the assumption holds.

Intuitively, we know that adding regularization into the error function should increase the bias and decrease the variance. However, before we consider the scenario, we need some mathematical tools from linear algebra. Let $A\in\mathbb{R}^{n\times n}$ be a square matrix with $n$ independent eigenvectors $\{q_1,\dots,q_n\}$ and let $\{s_1,\dots,s_n\}$ be its corresponding eigenvalues (i.e. $Aq_i=s_iq_i$). Then, $A$ can be decomposed into

$$
\newcommand{\vertbar}{\rule[-1ex]{0.5pt}{3.1ex}}
A=\underbrace{\begin{bmatrix}
\vertbar&\ &\vertbar\\
q_1 &\cdots&q_n\\
\vertbar&\ &\vertbar
\end{bmatrix}}_{Q}
\
\underbrace{\begin{bmatrix}
s_1&\ &\\
&\ddots& \\
&&s_n
\end{bmatrix}}_{S}
\
\underbrace{\begin{bmatrix}
\vertbar&\ &\vertbar\\
q_1 & \cdots & q_n\\
\vertbar&\ &\vertbar
\end{bmatrix}^{-1}}_{Q^{-1}}.
$$

where $Q=[q_1\ \cdots\ q_n]$ and $S=\operatorname{diag}(s_1,\dots,s_n)$. Furthermore, if $A$ has an eigenvalue $s$, then $A^{-1}$ has an eigenvalue $1/s$.

### Problem 3.2

Suppose $A$ can be decomposed into $QSQ^{-1}$. Show that $A+\lambda I$ can be decomposed into $QS'Q^{-1}$ where

$$
S'=\begin{bmatrix}
s_1+\lambda&\ &\\
&\ddots& \\
&&s_n+\lambda
\end{bmatrix}.
$$

### Problem 3.3

Let us get back to ridge regression. We previously derived that the predictor

$$y(x;D)=x^\top\underbrace{(X_D^\top X_D+\lambda I)^{-1}X_D^\top t_D}_{w_D}$$

minimizes the mean squared error with $L_2$ regularization $E(y,D)=E_{\text{mse}}(y,D)+E_{\text{reg}}(y,D)$. Assume $t(x)=w_*^\top x+\epsilon$ where $\epsilon\sim\mathcal{N}(0,1)$. Derive the bias term and comment on the effect of the regularization constant $\lambda$.

### Problem 3.4

(Optional) Derive the variance term and comment on the effect of the regularization constant $\lambda$.
"""