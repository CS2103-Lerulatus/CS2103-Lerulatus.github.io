{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS3244 Tutorial 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS3244 Tutorial 1"
      ],
      "metadata": {
        "id": "a4juiBvv9odS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem 1** - Moore-Penrose Pseudoinverse\n",
        "\n",
        "Recall that the pseudoinverse $X^+=(X^\\top X)^{-1}X^\\top$ appears as part of the analytic solution of linear regression. In this problem, we will explore some of the properties of this pseudoinverse.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GJUARvoR_YAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1.1\n",
        "\n",
        "Let $X\\in\\mathbb{R}^{N\\times D}$ and assume $X^\\top X$ is nonsingular.\n",
        "\n",
        "1. Show that $X^+X=I$.\n",
        "2. What is the dimension of $I$?"
      ],
      "metadata": {
        "id": "Vly_xPk4BtAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1.2\n",
        "\n",
        "The pseudoinverse $X^+=(X^\\top X)^{-1}X^\\top$ is also known as a left-inverse.\n",
        "\n",
        "1. Why do we call $X^+$ a left-inverse?\n",
        "2. What is a natural way to define a right-inverse (what do you need to assume with that definition)?"
      ],
      "metadata": {
        "id": "UwWW-1qiNgyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1.3\n",
        "\n",
        "Let $X\\in\\mathbb{R}^{N\\times N}$ and assume both $X^\\top X$ and $X$ are nonsingular.\n",
        "\n",
        "1. By considering $XX^+XX^{-1}$ or otherwise, show that $XX^+=I$.\n",
        "2. By considering $X^{-1}XX^+$ or otherwise, show that $X^+=X^{-1}$.\n",
        "\n",
        "In general, $XX^+\\neq I$."
      ],
      "metadata": {
        "id": "xQLasUhHDJh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem 2** - Vector Calculus\n",
        "\n",
        "Many modern machine learning techniques rely on using gradient information to search for a model. Typically, this involves differentiating a scalar function with respect to a vector. Take linear regression as an example: we differentiate the error function $E$ with respect to the parameters $w$. If $w\\in\\mathbb{R}^N$, then this is formally written as\n",
        "\n",
        "$$\\nabla_wE(w)=\\left[\\frac{\\partial E}{\\partial w_1}\\ \\cdots\\ \\frac{\\partial E}{\\partial w_N}\\right]^\\top.$$\n",
        "\n",
        "One method to derive $\\nabla_wE(w)$ is to compute the partial derivatives individually and combine it back into a vector. For example, given a function $f(w)=\\sum_i aw_i$ for some constant $a\\in\\mathbb{R}$, then the $i$-th partial derivative $\\partial f/\\partial w_i=a$. Hence\n",
        "\n",
        "$$\\nabla_wf(w)=\\left[a\\ \\cdots\\ a\\right]^\\top.$$\n",
        "\n",
        "In this problem, we shall derive some of the basic derivative rules in vector calculus. Although in practice, most people simply refer to the [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) for these rules, it is essential that you know how these rules are derived."
      ],
      "metadata": {
        "id": "ASqTcSnYMA0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.1\n",
        "\n",
        "Let $w\\in\\mathbb{R}^N$ and $x\\in\\mathbb{R}^N$. Consider the function $f(w)=w^\\top x$.\n",
        "\n",
        "1. For each $i\\in[N]$, show that the partial derivative $\\partial f/\\partial w_i=x_i$.\n",
        "2. Show that $\\nabla_wf(w)=x$."
      ],
      "metadata": {
        "id": "ZaMCNlwFTFA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.2\n",
        "\n",
        "Let $w\\in\\mathbb{R}^N$. Consider the function $f(w)=w^\\top w$.\n",
        "\n",
        "1. For each $i\\in[N]$, find the partial derivative $\\partial f/\\partial w_i$.\n",
        "2. Show that $\\nabla_wf(w)=2w$."
      ],
      "metadata": {
        "id": "W6SR-1UcXfa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.3\n",
        "\n",
        "Let $w\\in\\mathbb{R}^N$ and $g(w),h(w)$ be scalar function of $w$. Consider the function $f(w)=g(w)+h(w)$.\n",
        "\n",
        "1. For each $i\\in[N]$, find the partial derivative $\\partial f/\\partial w_i$.\n",
        "2. Show that $\\nabla_wf(w)=\\nabla_wg(w)+\\nabla_wh(w)$."
      ],
      "metadata": {
        "id": "vxIWBjnmYlgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.4*\n",
        "\n",
        "Let $w\\in\\mathbb{R}^N$ and $A\\in\\mathbb{R}^{N\\times N}$ such at $A^\\top=A$. Consider the function $f(w)=w^\\top Aw$.\n",
        "\n",
        "1. For each $i\\in[N]$, find the partial derivative $\\partial f/\\partial w_i$.\n",
        "2. Show that $\\nabla_wf(w)=2Aw$."
      ],
      "metadata": {
        "id": "r0wqRVJDZWLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2.5\n",
        "\n",
        "Recall that the error function for the L2-regularized linear regression can be written as\n",
        "\n",
        "$$E(w)=\\frac{1}{2}(t-Xw)^\\top(t-Xw)+\\frac{\\lambda}{2}\\lVert w\\rVert^2_2.$$\n",
        "\n",
        "Find $\\nabla_wE(w)$."
      ],
      "metadata": {
        "id": "kZ3AWD5zaD17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem 3** - Probablistic Inference\n",
        "\n",
        "A crucial goal of machine learning is to make inferences about a process based on the data collected from the same process. When deriving the error function of linear regression in lecture, this process is assumed to be the distribution $\\mathcal{N}(w^\\top x,\\sigma^2)$ and the data is assumed to be drawn from this process in an i.i.d. fashion. Since the inference is made using probablistic arguments, we call this a probablistic inference.\n",
        "\n",
        "In this problem, we shall find an optimal strategy for guessing the biasness of a coin. Suppose we are given a coin with unknown bias $q$. For each flip, we obtain heads with probability $q$ and tails with probability $1-q$. Our goal is to find a best guess for $q$, which we will denote as $\\hat{q}$.  "
      ],
      "metadata": {
        "id": "mDnY66g0cXLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3.1\n",
        "\n",
        "We flip the coin 10 times and observe $\\mathcal{D}=\\{H,T,T,H,T,H,H,H,T,H\\}$. Intuitively, we guess that $\\hat{q}=0.6$ because 6 out of the 10 flips are heads. Justify our guess with probablistic reasoning using the following guiding questions.\n",
        "\n",
        "1. Find the log-likelihood of observing the sequence of flips given that the true coin has a fixed bias $q$.\n",
        "2. Show that the $\\hat{q}$ that maximizes the likelihood is 0.6."
      ],
      "metadata": {
        "id": "c7JwyRSmsZ2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3.2\n",
        "\n",
        "Our coin provider later tell us that $q$ can only be either 0.3, 0.5 or 0.7. In other words, the space of all possible process shrinks from $\\mathcal{H}=[0, 1]$ to $\\mathcal{H}=\\{0.3,0.5,0.7\\}$. Find the maximum likelihood estimator(s) $\\hat{q}$."
      ],
      "metadata": {
        "id": "6gVOGh9twE46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3.3\n",
        "\n",
        "The coin provider then show us five coins, one of which has $q=0.3$, one of which has $q=0.5$, and three of which has $q=0.7$. After drawing exactly one of these five coins uniformly at random, you flip it 10 times and obtained the sequence $\\mathcal{D}=\\{H,T,T,H,T,H,H,H,T,H\\}$. Find the maximum a posteriori estimator(s) $\\hat{q}$."
      ],
      "metadata": {
        "id": "Pbpq3XeOxwFG"
      }
    }
  ]
}